{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "from collections import deque\n",
    "import csv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six import next\n",
    "from tensorflow.core.framework import summary_pb2\n",
    "import pandas as pd\n",
    "import dataio\n",
    "import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set varibales \n",
    "np.random.seed(13575)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "USER_NUM =  206202\n",
    "ITEM_NUM = 32\n",
    "DIM = 15\n",
    "EPOCH_MAX = 100\n",
    "DEVICE = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clip -ve values\n",
    "def clip(x):\n",
    "    return np.clip(x, 1.0, None)\n",
    "\n",
    "#create scalar summary of frame\n",
    "def make_scalar_summary(name, val):\n",
    "    return summary_pb2.Summary(value=[summary_pb2.Summary.Value(tag=name, simple_value=val)])\n",
    "\n",
    "#read data and split test-train\n",
    "def get_data():\n",
    "    df = dataio.read_process(\"data/user_basket_size.csv\", sep=\",\")\n",
    "    df['group_indicator'] = (df.ix[:,0] != df.ix[:,0].shift(-1)).astype(int)\n",
    "\n",
    "    df_train = df.loc[df.group_indicator==0]\n",
    "    df_train = df_train.drop('group_indicator', axis=1)\n",
    "\n",
    "    df_test =  df.loc[df.group_indicator==1]\n",
    "    df_test = df_test.drop('group_indicator', axis=1)\n",
    "    df = df.drop('group_indicator', axis=1)\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implementaion of SVD, create Tensorflow session\n",
    "def svd(train, test):\n",
    "    samples_per_batch = len(train) // BATCH_SIZE\n",
    "    print test.head(10)\n",
    "    iter_train = dataio.ShuffleIterator([train[\"user\"],\n",
    "                                         train[\"days_since_prior_order\"],\n",
    "                                         train[\"basket_size\"]],\n",
    "                                        batch_size=BATCH_SIZE)\n",
    "\n",
    "    iter_test = dataio.OneEpochIterator([test[\"user\"],\n",
    "                                         test[\"days_since_prior_order\"],\n",
    "                                         test[\"basket_size\"]],\n",
    "                                        batch_size=-1)\n",
    "\n",
    "    user_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_user\")\n",
    "    days_since_prior_order_batch = tf.placeholder(tf.int32, shape=[None], name=\"id_days_since_prior_order\")\n",
    "    basket_size_batch = tf.placeholder(tf.float32, shape=[None])\n",
    "\n",
    "    infer, regularizer = ops.inference_svd(user_batch, days_since_prior_order_batch, user_num=USER_NUM, item_num=ITEM_NUM, dim=DIM,\n",
    "                                           device=DEVICE)\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    _, train_op = ops.optimization(infer, regularizer, basket_size_batch, learning_rate=0.001, reg=0.05, device=DEVICE)\n",
    "\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=\"/tmp/svd/log\", graph=sess.graph)\n",
    "        errors = deque(maxlen=samples_per_batch)\n",
    "        start = time.time()\n",
    "        min = 100\n",
    "        predList = []\n",
    "        actList = []\n",
    "        finalPred = []\n",
    "        finalAct = []\n",
    "        finalpr = []\n",
    "        finalac = []\n",
    "        for i in range(EPOCH_MAX * samples_per_batch):\n",
    "\n",
    "            users, days_since_prior_orders, basket_sizes = next(iter_train)\n",
    "            _, pred_batch = sess.run([train_op, infer], feed_dict={user_batch: users,\n",
    "                                                                   days_since_prior_order_batch: days_since_prior_orders,\n",
    "                                                                   basket_size_batch: basket_sizes})\n",
    "            pred_batch = clip(pred_batch)\n",
    "            errors.append(np.power(pred_batch - basket_sizes, 2))\n",
    "            if i % samples_per_batch == 0:\n",
    "                train_err = np.sqrt(np.mean(errors))\n",
    "                test_err2 = np.array([])\n",
    "                for users, days_since_prior_orders, basket_sizes in iter_test:\n",
    "                    pred_batch = sess.run(infer, feed_dict={user_batch: users,\n",
    "                                                            days_since_prior_order_batch: days_since_prior_orders})\n",
    "                    #pred_batch = clip(pred_batch)\n",
    "                    test_err2 = np.append(test_err2, np.power(pred_batch - basket_sizes, 2))\n",
    "\n",
    "                    pr = pred_batch\n",
    "                    ac = basket_sizes\n",
    "                end = time.time()\n",
    "                test_err = np.sqrt(np.mean(test_err2))\n",
    "                train_err_summary = make_scalar_summary(\"training_error\", train_err)\n",
    "                test_err_summary = make_scalar_summary(\"test_error\", test_err)\n",
    "                summary_writer.add_summary(train_err_summary, i)\n",
    "                summary_writer.add_summary(test_err_summary, i)\n",
    "                start = end\n",
    "\n",
    "                if train_err < min:\n",
    "                    min = train_err\n",
    "                    finalpr = pr\n",
    "                    finalac = ac\n",
    "\n",
    "        return finalpr, finalac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    df_train, df_test = get_data()\n",
    "    pr, ac = svd(df_train, df_test)\n",
    "    prdf = pd.DataFrame(pr)\n",
    "    acdf = pd.DataFrame(ac)\n",
    "    result = pd.concat([prdf, acdf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pprint\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = pd.read_csv(\"data/orders_train_test.csv\")\n",
    "prior = pd.read_csv(\"data/order_products__prior.csv\")\n",
    "train = pd.read_csv(\"data/order_products__train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frames = [prior, train]\n",
    "products = result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orders = orders.loc[orders['group_indicator'] == 1]\n",
    "\n",
    "# find the products that were actually bought in the last order of every user\n",
    "test_orders = pd.merge(orders, products, on='order_id')\n",
    "\n",
    "test_orders2 = test_orders[['user_id', 'order_id', 'product_id']]\n",
    "\n",
    "# create a list of lists\n",
    "# list of the last order of each user\n",
    "# containing lists of the products that each user bought in his last order\n",
    "test_orders2 =  test_orders2.groupby(['user_id', 'order_id'])['product_id'].apply(list)\n",
    "#filename = 'actual_products.csv'\n",
    "#test_orders2.to_csv(filename, index=False, encoding='utf-8', header=False)\n",
    "\n",
    "\n",
    "test_set = pd.read_csv(\"data/test_set_.csv\", names = [\"user_id\", \"days_since_prior_order\", \"basket\", \"order_id\"])\n",
    "# the next dataset contains the predicted basket size of the next basket (output of svd_train_val.py)\n",
    "preds = pd.read_csv(\"data/pred-actual.csv\",  names = [\"pred\", \"actual\"])\n",
    "# this dataset contains statistics concerning users' consumer behaviour\n",
    "user_prod_stats = pd.read_csv(\"data/user_product_stats.csv\")\n",
    "#act_prods = pd.read_csv(\"data/actual_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preds = pd.concat([test_set, preds], axis=1)\n",
    "\n",
    "pred_prods =pd.DataFrame()\n",
    "l=int(len(test_set))\n",
    "c=int(1)\n",
    "final_pred_prods = []\n",
    "final_pred_prods2 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "# iterate through the dataframe containing the user_id and the predicted number of his next basket\n",
    "\n",
    "# for every user check the predicted size of his next basket and accordingly predict the products that he buy\n",
    "\n",
    "# the prediction of the next basket products depends on the following:\n",
    "# 1. predicted basket size\n",
    "# 2. the preferences of the user,\n",
    "sample_user = ''\n",
    "sample_size = ''\n",
    "sample_reco = ''\n",
    "for index, row in test_preds.iterrows():\n",
    "     user_stats = []\n",
    "     basket_size = int(round(row['pred'],0))\n",
    "     user = row['user_id']\n",
    "\n",
    "     user_stats = user_prod_stats.loc[user_prod_stats['user_id'] == user]\n",
    "     user_products = user_stats['product_id']\n",
    "\n",
    "     pred_prods  =  user_products.head(basket_size)\n",
    "     df_row = pred_prods.tolist()\n",
    "     sample_user = user\n",
    "     sample_size = basket_size\n",
    "     sample_reco = pred_prods\n",
    "     final_pred_prods.append(df_row)\n",
    "\n",
    "     i = i+1\n",
    "\n",
    "print sample_user\n",
    "print sample_size\n",
    "print sample_reco\n",
    "# print type(final_pred_prods)\n",
    "# print type(final_pred_prods[1])\n",
    "\n",
    "# print 'results'\n",
    "# for xs in final_pred_prods:\n",
    "#     print \",\".join(map(str, xs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a list of lists\n",
    "# list of the last order of each user\n",
    "# containing lists of the predicted products for this order\n",
    "with open('data/pred_products.csv', 'wb') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "    #wr = \",\".join(map(str, wr))\n",
    "    wr.writerow(final_pred_prods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "\n",
    "#preds = pd.read_csv(\"pred_products2.csv\")\n",
    "predss = []\n",
    "\n",
    "\n",
    "# read CSV file & load into list\n",
    "with open(\"data/pred_products.csv\", 'r') as my_file:\n",
    "    reader = csv.reader(my_file, delimiter='\\t')\n",
    "    preds = list(reader)\n",
    "\n",
    "with open(\"data/actual_products.csv\", 'r') as my_file:\n",
    "    reader = csv.reader(my_file, delimiter='\\t')\n",
    "    acts = list(reader)\n",
    "\n",
    "acts = [l[0] for l in acts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TTP = 0\n",
    "TFP = 0\n",
    "TFN = 0\n",
    "TT = 0\n",
    "\n",
    "i= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pred, act in zip(preds, acts):\n",
    "        act = str(act)\n",
    "        act = act.replace(\" \", \"\")\n",
    "        pred = str(pred)\n",
    "\n",
    "        pred = pred.replace(\" \", \"\")\n",
    "        pred = pred.replace(\"'\", \"\")\n",
    "        pred = pred.replace(\"[\", \"\")\n",
    "        pred = pred.replace(\"]\", \"\")\n",
    "        act = act.replace(\"[\", \"\")\n",
    "        act = act.replace(\"]\", \"\")\n",
    "\n",
    "        act = act.split(\",\")\n",
    "        pred = pred.split(\",\")\n",
    "\n",
    "\n",
    "        pred = set(pred)\n",
    "        act = set(act)\n",
    "\n",
    "        TP = len(set.intersection(act, pred))\n",
    "\n",
    "        UN = len(set.union(act, pred))\n",
    "\n",
    "        FP = len(pred)-TP\n",
    "        FN = len(act)-TP\n",
    "        T = len(act)\n",
    "\n",
    "        AC = TP/float(T)\n",
    "        #print TP, UN, FP, FN, T\n",
    "        TTP=TTP+TP\n",
    "        TFP=TFP+FP\n",
    "        TFN=TFN+FN\n",
    "        TT=TT+T\n",
    "        #print TTP, TFP, TFN, TT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TAC = TTP/float(TT)\n",
    "#print TTP,TT\n",
    "PRE = TTP/float((TTP+TFP))\n",
    "REC = TTP/float((TTP+TFN))\n",
    "F1 = (2*(PRE*REC))/float((PRE+REC))\n",
    "\n",
    "i = i+1\n",
    "print 'true positives', TTP, '\\nfalse positives', TFP, '\\nfalse negatives', TFN, '\\ntotal products bought', TT\n",
    "print '\\naccuracy', TAC, '\\nprecision', PRE, '\\nrecall', REC, '\\nf1', F1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
